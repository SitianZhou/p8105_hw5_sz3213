---
title: "p8105 hw5"
author: "Sitian Zhou"
date: "2023-11-05"
output: github_document
---

```{r, message=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	warning = FALSE,
	message = FALSE,
	fig.width = 12, 
	fig.height = 9)

theme_set(theme_minimal())
```



## Problem 1

```{r}
homicide <-
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, ", ", state),
    resolved = as.numeric(disposition == "Closed by arrest")
  ) |> 
  filter(city_state != "Tulsa, AL") 
```
The dataset `homicide` contains information about homicide cases in United States from 2007 to 2017. It contains `r nrow(homicide)` rows and `r ncol(homicide)` variable, where each row is a criminal homicide case. The dataset includes the time the case was reported, the information about victims, the location of killing, and whether an arrest was made. 

The following codes create tables that summarize the total number of homicides and the number of unsolved homicides across cities.
```{r}
# total number of homicides
homicide |> 
  count(city_state) |> 
  knitr::kable()
# number of unsolved homicides
homicide |> 
  filter(resolved == 0) |> 
  count(city_state) |> 
  knitr::kable()
```

The estimated proportion of unresolved homicides and its confidence interval in Baltimore.

```{r}
homicide_summary <-
  homicide %>% 
  group_by(city_state) %>% 
  summarize(
    case_total = n(),
    case_unsolved = sum(resolved == 0))


prop_res_Baltimore <-
  prop.test(
    x = filter(homicide_summary, city_state == "Baltimore, MD") |> pull(case_unsolved),
    n = filter(homicide_summary, city_state == "Baltimore, MD") |> pull(case_total))

prop_res_Baltimore|>
  broom::tidy() |> 
  select(estimate, conf.low, conf.high) |> 
  knitr::kable(digits = 3)
```


```{r}
prop_res = 
  homicide_summary |> 
  mutate(
    prop_tests = map2(case_unsolved, case_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |> 
  select(-prop_tests) |>  
  unnest(tidy_tests) |>  
  select(city_state, estimate, conf.low, conf.high) |> 
  mutate(city_state = fct_reorder(city_state, estimate))
```



```{r}
prop_res |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = estimate, y = city_state)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high))

```


## Problem 2


```{r}
files = list.files(path = "data", pattern = "*_[0-9]")
file_name = str_c("data/",files)

```


```{r}
patient_id = rep(1:10, times = 2)
arm = rep(c("con","exp"),each = 10)

res_df <-
  tibble(
    patient_id = patient_id,
    arm = arm
  )

# loading datasets
res_df <-
  res_df |> 
  mutate(
    data = map(file_name, read_csv)
  ) |> 
  unnest()

# tidy
res_df_clean <-
  res_df |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "value"
  ) |> 
  mutate(
    week = as.numeric(week),
    patient_id = as.factor(patient_id)
  )


```


```{r}
ggplot(data = res_df_clean, aes(x = week, y = value, color = patient_id)) +
  geom_line() +
  facet_grid(arm ~ .) +
  labs(title = "Values for each patient in control and experimental arms over time")
```

### comment on differences between groups

The values of ten participants in the control arm remain stable across eight weeks, while in the experimental arm, the value increases across eight weeks.



## Problem 3

```{r}
sim_t_test = function(mu, n = 30, sd = 5) {
  x_vec = rnorm(n, mu, sd)
  t.test(x_vec) |> 
  broom::tidy() |> 
  select(
    mu_hat = estimate,
    p_val = p.value
  )
  
}

# mu = 0
output = vector("list", length = 5000)

for (i in 1:5000) {
  output[[i]] = sim_t_test(0)
}

sim_res <-
  bind_rows(output)

# simulation for mu = 1:6
sim_res_df = 
  expand_grid(
    mu = c(1, 2, 3, 4, 5, 6), 
    iter = 1:5000
  ) |> 
  mutate(
    est_df = map(mu, sim_t_test)
  ) |> 
  unnest(cols = c(est_df))

# plot the results
sim_res_df |> 
  group_by(mu) |> 
  summarize(rej_prop = sum(p_val < 0.05) / 5000) |> 
  ggplot(aes(x = mu, y = rej_prop)) +
  geom_point() + geom_line() +
  labs(title = "Reject proportion across true mean", y = "reject proportion")
```

As the effect size increases, the rejection proportion increases and finally reaches 100% for $\mu$ = 5 and $\mu$ = 6. This indicates the power of the test increases and finally reaches to 1 when the effect size increases.


```{r}
estimate_df <-
  sim_res_df |> 
  group_by(mu) |> 
  summarize(mean_mu_hat = mean(mu_hat))


estimate_rej_df <-
  sim_res_df |> 
  mutate(
    decision = ifelse(p_val < 0.05, "rej", "fail")
  ) |> 
  filter(decision == "rej") |> 
  group_by(mu, decision) |> 
  summarize(mean_mu_hat = mean(mu_hat))

ggplot(estimate_df, aes(x = mu, y = mean_mu_hat, color = "black")) + geom_line() + geom_point() +
  geom_line(data = estimate_rej_df, aes(color = "red")) + 
  geom_point(data = estimate_rej_df, aes(color = "red")) +
  labs(title = "Total estimates mean vs. rejected estimates mean",
       y = "estimates mean") +
  scale_color_manual(name = "Legend", 
                     values = c("black" = "black", "red" = "red"),
                     labels = c('total estimates','rejected estimates'))

```



The sample mean of $\hat{\mu}$ for tests with null rejected is greater than the true $\mu$ when the effect size is small ($\mu$ = 1 or 2). Small effect size is also associated with low rejection proportion, which implies a large proportion of samples have means close to 0. This further implies the mean for the rest of the samples will be greater than the true mean.  As true $\mu$ increases, the sample mean of $\hat{\mu}$ for tests with null rejected gets closer to the true $\mu$. This aligns with the behavior of rejection proportion for large effect size. 




